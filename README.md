# Konveyor AI Evaluation Framework

A comprehensive framework for evaluating LLM performance on application modernization tasks. Automatically generates test cases from Konveyor rules, validates code migrations, and produces professional reports with migration complexity analysis.

## âœ¨ Key Features

- ğŸ¤– **Automatic test generation** from 2,680+ Konveyor rules with LLM code generation
- ğŸ“Š **Migration complexity classification** (TRIVIAL â†’ EXPERT) with expected AI success rates
- ğŸ”„ **Agentic workflows** for self-correcting code generation and fixing
- ğŸ“ˆ **Professional HTML reports** with Grafana-style design and complexity breakdowns
- ğŸ¯ **Multi-dimensional evaluation**: Functional correctness, security, code quality, explainability
- âš¡ **Graceful interruption** with progress saving and resume capability
- ğŸ” **Real Java compilation** with 200+ JARs from Maven Central

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure API keys
cp config.example.yaml config.yaml
# Edit config.yaml with your OpenAI/Anthropic API keys

# 3. Setup Java dependencies (one-time)
cd evaluators/stubs && mvn clean package && cd ../..

# 4. Generate test suite with auto-generated code
python scripts/generate_tests.py --all-rulesets --target quarkus \
  --auto-generate --validate --model gpt-4-turbo

# 5. Classify by complexity
python scripts/classify_rule_complexity.py benchmarks/test_cases/generated/quarkus.yaml

# 6. Run evaluation
python evaluate.py --benchmark benchmarks/test_cases/generated/quarkus.yaml

# 7. View results
open results/evaluation_report_*.html
```

**See [WORKFLOW.md](WORKFLOW.md) for the complete recommended workflow.**

## ğŸ“‹ Migration Complexity Classification

Automatically categorizes rules by difficulty to set appropriate AI expectations:

| Complexity | AI Success | Example | Automation Strategy |
|-----------|-----------|---------|---------------------|
| **TRIVIAL** | 95%+ | `javax.*` â†’ `jakarta.*` namespace changes | âœ… Full automation |
| **LOW** | 80%+ | `@Stateless` â†’ `@ApplicationScoped` | âœ… Automation with light review |
| **MEDIUM** | 60%+ | JMS â†’ Reactive Messaging patterns | âš¡ AI acceleration + developer completion |
| **HIGH** | 30-50% | Spring Security â†’ Quarkus Security | ğŸ” AI scaffolding + expert review |
| **EXPERT** | <30% | Custom security realms, internal APIs | ğŸ“‹ AI checklist + human migration |

**Why it matters:** Segmented analysis reveals AI value at each difficulty level, not just overall pass rate.

See [docs/COMPLEXITY_CLASSIFICATION.md](docs/COMPLEXITY_CLASSIFICATION.md) for details.

## ğŸ“¦ Test Generation

Generate test cases from Konveyor rulesets with automatic code generation:

```bash
# Generate from specific migration path (Java EE â†’ Quarkus)
python scripts/generate_tests.py --all-rulesets --source java-ee --target quarkus \
  --auto-generate --validate --model gpt-4-turbo

# With local rulesets (100x faster, no API rate limits)
git clone https://github.com/konveyor/rulesets.git ~/projects/rulesets
python scripts/generate_tests.py --all-rulesets --target quarkus \
  --local-rulesets ~/projects/rulesets \
  --auto-generate --validate --model gpt-4-turbo

# Process in batches for large test suites
python scripts/generate_tests.py --all-rulesets --target quarkus \
  --auto-generate --model gpt-4-turbo \
  --batch-size 20
```

**Features:**
- âœ… Auto-generates `code_snippet` and `expected_fix` using LLM
- âœ… `--validate` flag: Agentic workflow compiles and auto-fixes errors
- âœ… **Ctrl+C** to pause and save progress - resume by re-running same command
- âœ… Auto-detects language from rules (Java, XML, YAML, properties)
- âœ… Includes Konveyor migration guidance in prompts

**Output:** `benchmarks/test_cases/generated/quarkus.yaml`

See [docs/generating_tests.md](docs/generating_tests.md) for full documentation.

## ğŸ”§ Test Validation & Fixing

### Validate Test Cases

```bash
# Check compilation status
python scripts/validate_expected_fixes.py benchmarks/test_cases/generated/quarkus.yaml
```

### Auto-Fix Compilation Errors

```bash
# Fix by complexity level (recommended: start with TRIVIAL/LOW)
python scripts/fix_expected_fixes.py \
  --file benchmarks/test_cases/generated/quarkus.yaml \
  --complexity trivial,low

# Preview fixes without applying
python scripts/fix_expected_fixes.py \
  --file benchmarks/test_cases/generated/quarkus.yaml \
  --dry-run
```

**Features:**
- âœ… Agentic workflow: Iterates with compilation error feedback (max 3 attempts)
- âœ… Migration-specific guidance (Java EE â†’ Quarkus patterns)
- âœ… **Ctrl+C** to pause - progress saved incrementally
- âœ… Auto-skips non-compilable tests (marked with reasons)
- âœ… Validates each fix before applying

See [docs/automating_test_case_fixes.md](docs/automating_test_case_fixes.md) for details.

## ğŸ“Š Evaluation & Reports

### Run Evaluation

```bash
# Evaluate all complexity levels
python evaluate.py --benchmark benchmarks/test_cases/generated/quarkus.yaml

# Filter by complexity
python evaluate.py \
  --benchmark benchmarks/test_cases/generated/quarkus.yaml \
  --filter-complexity trivial,low

# Exclude EXPERT level
python evaluate.py \
  --benchmark benchmarks/test_cases/generated/quarkus.yaml \
  --exclude-complexity expert

# Parallel execution for multiple models
python evaluate.py \
  --benchmark benchmarks/test_cases/generated/quarkus.yaml \
  --parallel 4
```

### HTML Reports

**[ğŸ“Š View Live Sample Report](https://tsanders-rh.github.io/konveyor-iq/)** - Interactive demo (no installation required)

Professional Grafana-style reports with:

- ğŸ¨ **Dark theme** with Konveyor branding
- ğŸ“Š **Complexity breakdown table** - Pass rates by difficulty level (TRIVIAL â†’ EXPERT)
- ğŸ† **Model ranking** with composite scoring across all dimensions
- ğŸ” **Individual test cards** with complexity badges
- ğŸ“ˆ **Interactive charts** - Response time, cost analysis, performance metrics
- ğŸ”’ **Security analysis** - Vulnerability detection with severity levels
- ğŸ’¡ **Explainability metrics** - Comment density, explanation quality scores
- âš ï¸ **Detailed failure analysis** - Diff highlighting, compilation errors

**Output:** `results/evaluation_report_*.html`

## ğŸ¯ Evaluation Dimensions

Tests evaluate LLM performance across:

1. **Functional Correctness** (40% weight)
   - Compiles successfully
   - Resolves original violation
   - No new violations introduced

2. **Compilation Rate** (15% weight)
   - Valid Java syntax
   - Correct imports and dependencies

3. **Code Quality** (15% weight)
   - Cyclomatic complexity
   - Maintainability index
   - Style adherence

4. **Security** (15% weight)
   - No SQL injection vulnerabilities
   - No hardcoded credentials
   - Migration-specific checks (lost @RolesAllowed, etc.)
   - Optional Semgrep integration for comprehensive analysis

5. **Explainability** (10% weight)
   - Explanation quality score (0-10)
   - Comment density (10-30% ideal)

6. **Performance Metrics** (5% weight)
   - Response time
   - Cost efficiency

## ğŸ”’ Security Analysis

Hybrid approach for comprehensive security scanning:

```bash
# Install Semgrep (optional but recommended)
pip install semgrep

# Uncomment in config.yaml:
security:
  tools:
    - semgrep
```

**Without Semgrep:** Fast pattern-based checks for common vulnerabilities
**With Semgrep:** Comprehensive taint analysis and framework-specific rules

## ğŸ“ Project Structure

```
konveyor-iq/
â”œâ”€â”€ benchmarks/              # Test datasets
â”‚   â””â”€â”€ test_cases/
â”‚       â””â”€â”€ generated/       # Auto-generated from Konveyor rules
â”œâ”€â”€ config/                  # Configuration files
â”‚   â””â”€â”€ migration_guidance.yaml  # Migration-specific prompts
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ SETUP.md
â”‚   â”œâ”€â”€ COMPLEXITY_CLASSIFICATION.md
â”‚   â”œâ”€â”€ generating_tests.md
â”‚   â””â”€â”€ automating_test_case_fixes.md
â”œâ”€â”€ evaluators/              # Metric implementations
â”‚   â”œâ”€â”€ functional.py        # Compilation & pattern validation
â”‚   â”œâ”€â”€ security.py          # Security analysis
â”‚   â”œâ”€â”€ quality.py           # Code quality metrics
â”‚   â””â”€â”€ stubs/              # Java compilation dependencies
â”‚       â”œâ”€â”€ pom.xml         # Maven dependencies (200+ JARs)
â”‚       â””â”€â”€ lib/            # Downloaded JARs
â”œâ”€â”€ models/                  # LLM adapters
â”‚   â”œâ”€â”€ openai_adapter.py
â”‚   â”œâ”€â”€ anthropic_adapter.py
â”‚   â””â”€â”€ google_adapter.py
â”œâ”€â”€ reporters/               # Report generation
â”‚   â””â”€â”€ html_reporter.py    # Grafana-style HTML reports
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ generate_tests.py   # Generate from Konveyor rules
â”‚   â”œâ”€â”€ classify_rule_complexity.py  # Classify difficulty
â”‚   â”œâ”€â”€ validate_expected_fixes.py   # Validate compilation
â”‚   â””â”€â”€ fix_expected_fixes.py       # Auto-fix errors
â”œâ”€â”€ config.yaml              # Main configuration
â”œâ”€â”€ evaluate.py              # Main evaluation script
â”œâ”€â”€ WORKFLOW.md              # Recommended workflow
â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸ“š Documentation

- **[WORKFLOW.md](WORKFLOW.md)** - Complete workflow from generation to evaluation
- **[WORKFLOW_GENERATING_TESTS.md](WORKFLOW_GENERATING_TESTS.md)** - Technical architecture and JAR system
- **[docs/SETUP.md](docs/SETUP.md)** - Installation and configuration
- **[docs/COMPLEXITY_CLASSIFICATION.md](docs/COMPLEXITY_CLASSIFICATION.md)** - Classification system details
- **[docs/generating_tests.md](docs/generating_tests.md)** - Test generation guide
- **[docs/automating_test_case_fixes.md](docs/automating_test_case_fixes.md)** - Auto-fixing workflow
- **[docs/validating_expected_fixes.md](docs/validating_expected_fixes.md)** - Validation guide

## ğŸ› ï¸ Advanced Features

### Agentic Workflows

Both test generation and fixing use agentic workflows for self-correction:

1. **Generate code** â†’ Compile â†’ Get errors
2. **Fix errors with LLM** (passes error feedback)
3. **Validate fix** â†’ Compile â†’ Repeat if needed (max 3 attempts)

This dramatically improves compilation success rates (50% â†’ 85%+).

### Graceful Interruption

Press **Ctrl+C** (or Cmd+C) to pause long-running scripts:
- Finishes current operation
- Saves all progress to YAML
- Shows summary
- Resume by re-running same command

Supported in: `generate_tests.py`, `fix_expected_fixes.py`

### Migration Guidance System

Centralized, technology-specific guidance in `config/migration_guidance.yaml`:

- Java EE â†’ Quarkus patterns
- Spring Boot â†’ Quarkus patterns
- Messaging migrations
- Security migrations

Automatically injected into prompts based on test suite metadata.

### Konveyor Rule Integration

When test cases reference Konveyor rules via `source` URL:
1. Framework fetches official rule from GitHub
2. Extracts migration guidance (`message` field)
3. Includes in LLM prompt under "Konveyor Migration Guidance"

Ensures LLM receives authoritative migration patterns.

## ğŸ”§ Configuration

Edit `config.yaml` to specify:
- **Models**: OpenAI (GPT-4, GPT-3.5), Anthropic (Claude), Google (Gemini)
- **Evaluation dimensions**: Enable/disable metrics
- **Security tools**: Pattern-based or Semgrep
- **Report format**: HTML, Markdown
- **Parallelization**: Number of concurrent workers

## ğŸ“ Example Results

```
Pass Rate by Complexity (gpt-4-turbo):
- TRIVIAL: 96% (24/25 tests)
- LOW:     84% (21/25 tests)
- MEDIUM:  68% (17/25 tests)
- HIGH:    44% (11/25 tests)
- EXPERT:  20% (5/25 tests)

Overall: 78% (78/125 tests)
```

This segmentation demonstrates AI value at each difficulty level.

## ğŸ¤ Contributing

Contributions welcome! Areas of interest:
- New migration patterns for `evaluators/functional.py`
- Additional migration guidance for `config/migration_guidance.yaml`
- Enhanced security rules
- New LLM adapters

## ğŸ“„ License

Apache 2.0

---

**Quick Links:**
- [WORKFLOW.md](WORKFLOW.md) - Recommended workflow
- [docs/SETUP.md](docs/SETUP.md) - Installation guide
- [docs/COMPLEXITY_CLASSIFICATION.md](docs/COMPLEXITY_CLASSIFICATION.md) - Classification details
