# Konveyor AI Evaluation Framework

A comprehensive evaluation framework for assessing LLM-generated code fixes for rule-based static analysis violations.

## Overview

This framework evaluates LLM performance across multiple dimensions:
- **Functional Correctness**: Does the fix resolve the violation and compile/run?
- **Code Quality**: Readability, style adherence, complexity metrics
- **Security & Safety**: Avoids insecure patterns
- **Efficiency**: Computational performance
- **Explainability**: Quality of explanations/comments
- **Robustness**: Consistency across prompt variations
- **Response Time**: Latency metrics

## Project Structure

```
konveyor-iq/
â”œâ”€â”€ benchmarks/              # Test datasets
â”‚   â”œâ”€â”€ rules/              # Rule definitions
â”‚   â””â”€â”€ test_cases/         # Violation test cases
â”œâ”€â”€ evaluators/             # Metric implementations
â”‚   â”œâ”€â”€ functional.py       # Functional correctness checks
â”‚   â”œâ”€â”€ quality.py          # Code quality metrics
â”‚   â”œâ”€â”€ security.py         # Security analysis
â”‚   â”œâ”€â”€ efficiency.py       # Performance metrics
â”‚   â””â”€â”€ explainability.py   # Explanation quality
â”œâ”€â”€ models/                 # LLM adapters
â”‚   â”œâ”€â”€ base.py            # Base model interface
â”‚   â”œâ”€â”€ openai_adapter.py  # OpenAI models
â”‚   â”œâ”€â”€ anthropic_adapter.py # Claude models
â”‚   â””â”€â”€ local_adapter.py    # Local/HF models
â”œâ”€â”€ reporters/              # Report generation
â”‚   â”œâ”€â”€ html_reporter.py   # HTML dashboard
â”‚   â”œâ”€â”€ markdown_reporter.py # Markdown reports
â”‚   â””â”€â”€ templates/         # Report templates
â”œâ”€â”€ results/                # Evaluation outputs (gitignored)
â”œâ”€â”€ config.yaml             # Configuration
â”œâ”€â”€ evaluate.py             # Main evaluation script
â””â”€â”€ requirements.txt        # Python dependencies
```

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Configure models and benchmarks
cp config.example.yaml config.yaml
# Edit config.yaml with your API keys and settings

# Run evaluation
python evaluate.py --benchmark benchmarks/test_cases/java-migration.yaml

# Generate report
python evaluate.py --report results/latest/
```

## Adding Test Cases

Create YAML files in `benchmarks/test_cases/`:

```yaml
test_suite: "Java EE to Quarkus Migration"
rules:
  - rule_id: "java-deprecated-api-001"
    description: "Replace javax.ejb with CDI annotations"
    severity: "high"
    test_cases:
      - id: "tc001"
        code_snippet: |
          @Stateless
          public class UserService {
              // code
          }
        expected_fix: |
          @ApplicationScoped
          public class UserService {
              // code
          }
        context: "EJB to CDI migration"
        expected_metrics:
          functional_correctness: true
          introduces_violations: false
```

## Configuration

Edit `config.yaml` to specify:
- Models to evaluate
- Evaluation dimensions to include
- Static analysis tools to use
- Report format preferences

## Reports

### HTML Reports (Interactive)
- ğŸ“Š Model comparison charts with Plotly
- ğŸ“ˆ Response time distributions
- ğŸ¯ Per-rule performance breakdown
- ğŸ” **Interactive failure analysis**:
  - Click-to-expand failure cards
  - Side-by-side code comparison
  - Filter by failure type (compilation, regression, security)
  - Color-coded badges for quick identification
  - Detailed metrics per failure

### Markdown Reports
- Per-model performance summary
- Per-rule accuracy breakdown
- Comparative analysis across models
- Failure examples with code snippets
- Cost analysis (tokens/requests)

## License

Apache 2.0
